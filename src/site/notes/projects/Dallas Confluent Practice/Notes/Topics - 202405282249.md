---
{"dg-publish":true,"permalink":"/projects/dallas-confluent-practice/notes/topics-202405282249/","tags":["‚ô£Ô∏è/kafka","üìñ"],"created":"2025-05-12T23:14:32.000-05:00","updated":"2025-01-25T10:21:24.000-06:00"}
---


# Topics

A particular stream of data. Like a table in a database without all the constraints. You can have as many topics as you want. A topic is identified by its name.

Any kind of message format, JSON, [[Avro \|Avro ]], etc.

The sequence of messages is called a Data Stream. You cannot query topics directly through Kafka, you will send data using Producers and read data through Consumers.

Topics are [[projects/Domain-driven Design/Immutable - 202302170858\|Immutable]].  
Data in a topic is kept only for a limited time (default is one week but is configurable.)

Once a topic is created, the [[projects/Dallas Confluent Practice/Notes/Message Serializer - 202405302131\|Message Serializer]] /[[projects/Dallas Confluent Practice/Notes/Consumer Deserializer - 202405302142\|Consumer Deserializer]] type must not change during a topic lifecycle (create a new topic instead).

Topics are made up of [[projects/Dallas Confluent Practice/Notes/Partitions - 202405282258\|Partitions]].

## References


<div class="transclusion internal-embed is-loaded"><a class="markdown-embed-link" href="/references/highlights/kafka-the-definitive-guide-v2-highlights/#ref-745647014" aria-label="Open link"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon lucide-link"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a><div class="markdown-embed">



>[!QUOTE]  
>Messages in Kafka are categorized into topics. The closest analogies for a topic are a database table or a folder in a filesystem (Page¬†5) #‚úÇÔ∏è 

</div></div>



<div class="transclusion internal-embed is-loaded"><a class="markdown-embed-link" href="/references/highlights/kafka-the-definitive-guide-v2-highlights/#ref-745647017" aria-label="Open link"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon lucide-link"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a><div class="markdown-embed">



>[!QUOTE]  
>The term stream is often used when discussing data within systems like Kafka. Most often, a stream is considered to be a single topic of data, regardless of the number of partitions. This represents a single stream of data moving from the producers to the consumers. (Page¬†6) #‚úÇÔ∏è 

</div></div>


## Flashcards

Topics::: A particular stream of data.
<!--SR:!2025-01-28,4,299!2025-01-25,4,293-->

<!--SR:!2024-08-03,37,290!2024-08-18,52,310-->

Topics can contain these message formats::: JSON, AVRO, Binary, anything
<!--SR:!2025-02-11,17,311!2025-01-27,4,298-->

<!--SR:!2024-09-09,62,310!2024-09-06,59,310-->

To query a topic::: You can't do this directly, you send data using [[projects/Dallas Confluent Practice/Notes/Producer - 202405302120\|Producers]] and read data through [[projects/Dallas Confluent Practice/Notes/Consumer - 202405302133\|Consumers]]
<!--SR:!2025-02-10,16,312!2025-01-27,3,279-->

<!--SR:!2024-08-06,40,290!2024-08-30,64,310-->

A sequence of messages::: [[Data stream\|Data stream]]
<!--SR:!2000-01-01,1,250!2025-01-25,4,293-->

<!--SR:!2024-09-02,55,310!2024-09-18,71,310-->

Topics are ==[[projects/Domain-driven Design/Immutable - 202302170858\|Immutable]]== and cannot change  
The default retention period for a topic is:: One week
<!--SR:!2025-02-07,13,296-->

A topic's messages serialization type must not:: change during a topic's lifecycle
<!--SR:!2025-01-29,4,279-->

<!--SR:!2024-08-29,63,310-->

If a topic has a replication factor of 3...:: Each partition will live on 3 different brokers
<!--SR:!2025-01-25,4,296-->

<!--SR:!2024-09-04,57,316-->

There are 3 brokers. A kafka topic has a replication factor of 3 and min.insync.replicas setting of 1. What is the maximum number of brokers that can be down so that a producer with acks=all can still produce to the topic?:: Two brokers can go down, and one replica will still be able to receive and serve data
<!--SR:!2025-01-28,4,299-->

<!--SR:!2024-07-31,22,296-->

A client connects to a broker in the cluster and sends a fetch request for a partition in a topic. It gets an exception NotLeaderForPartitionException in the response. How does client handle this situation?:: Send metadata request to the same broker for the topic and select the broker hosting the leader replica. In case the consumer has the wrong leader of a partition, it will issue a metadata request. The Metadata request can be handled by any node, so clients know afterwards which broker are the designated leader for the topic partitions. Produce and consume requests can only be sent to the node hosting partition leader.
<!--SR:!2025-02-07,13,296-->

<!--SR:!2024-07-31,34,296-->

Your topic is log compacted and you are sending a message with the key K and value null. What will happen?:: The broker will delete all messages with the key K upon cleanup. Sending a message with the null value is called a tombstone in Kafka and will ensure the log compacted topic does not contain any messages with the key K upon compaction
<!--SR:!2025-01-28,4,299-->

<!--SR:!2024-09-24,77,329-->

When auto.create.topics.enable is set to true in Kafka configuration, what are the circumstances under which a Kafka broker automatically creates a topic? (select three)
- Producer sends message to a topic
- Client requests metadata for a topic
- Consumer reads message from a topic
- Client alters number of partitions of a topic
?
- Producer sends message to a topic
- Client requests metadata for a topic
- Consumer reads message from a topic
A kafka broker automatically creates a topic under the following circumstances: - When a producer starts writing messages to the topic - When a consumer starts reading messages from the topic - When any client requests metadata for the topic
<!--SR:!2025-01-25,5,269-->

Compaction is enabled for a topic in Kafka by setting log.cleanup.policy=compact. What is true about log compaction?:: After cleanup, only one message per key is retained with the latest value. Log compaction retains at least the last known value for each record key for a single topic partition. All compacted log offsets remain valid, even if record at offset has been compacted away as a consumer will get the next highest offset.
<!--SR:!2025-01-28,4,299-->

<!--SR:!2024-08-17,39,309-->

What is true about partitions? (select two)
- A broker can have a partition and its replica on its disk
- You cannot have more partitions than the number of brokers in your cluster
- Only out of sync replicas are replicas, the remaining partitions that are in sync are also leader
- A partition has one replica that is a leader, while the other replicas are followers
- A broker can have different partitions numbers for the same topic on its disk
?
- A partition has one replica that is a leader, while the other replicas are followers
- A broker can have different partitions numbers for the same topic on its disk
Only one of the replicas is elected as partition leader. And a broker can definitely hold many partitions from the same topic on its disk, try creating a topic with 12 partitions on one broker!
<!--SR:!2025-02-23,34,309-->

What is true about replicas?
- Produce and consume requests are load-balanced between Leader and Follower replicas
- Follower replica handles all consume requests
- Produce requests can be done to the replicas that are followers
- Leader replica handles all produce and consume requests
?
Leader replica handles all produce and consume requests. Replicas are passive - they don't handle produce or consume request. Produce and consume requests get sent to the node hosting partition leader.
<!--SR:!2025-03-01,40,316-->

How will you set the retention for the topic named "my-topic" to 1 hour?:: Set the topic config retention.ms to 3600000. retention.ms can be configured at topic level while creating topic or by altering topic. It shouldn't be set at the broker level (log.retention.ms) as this would impact all the topics in the cluster, not just the one we are interested in
<!--SR:!2025-01-26,1,239-->

<!--SR:!2024-08-27,49,312-->

There are 3 brokers in the cluster. You want to create a topic with a single partition that is resilient to one broker failure and one broker maintenance. What is the replication factor will you specify while creating the topic?:: 3. 1 is not possible as it doesn't provide resilience to failure, 2 is not enough as if we take a broker down for maintenance, we cannot tolerate a broker failure, and 6 is impossible as we only have 3 brokers (RF cannot be greater than the number of brokers). Here the correct answer is 3
<!--SR:!2025-01-26,3,278-->

<!--SR:!2024-07-19,10,296-->

By default, which replica will be elected as a partition leader? (select two)
- Preferred leader broker if it is in-sync and auto.leader.rebalance.enable=false
- Any of the replicas
- Preferred leader broker if it is in-sync and auto.leader.rebalance.enable=true
- An in-sync replica
?
- Preferred leader broker if it is in-sync and auto.leader.rebalance.enable=true
- An in-sync replica
Preferred leader is a broker that was leader when topic was created. It is preferred because when partitions are first created, the leaders are balanced between brokers. Otherwise, any of the in-sync replicas (ISR) will be elected leader, as long as unclean.leader.election=false (by default)'
<!--SR:!2025-02-16,27,296-->

What physically is Kafka partitions made of?:: One file and 2 indexes per segment. Kafka partitions are made of segments (usually each segment is 1GB), and each segment has two corresponding indexes (offset index and time index)
<!--SR:!2025-01-27,3,279-->

<!--SR:!2024-07-17,2,196-->

Which of the following statements are true regarding the number of partitions of a topic?
- We can add partitions in a topic using the kafka-topics.sh command
- We can remove partitions in a topic using the kafka-topics.sh command
- The number of partitions in a topic cannot be altered
- We can add partitions in a topic by adding a broker to the cluster
- We can remove partitions in a topic by removing a broker
?
We can only add partitions to an existing topic, and it must be done using the kafka-topics.sh command
<!--SR:!2025-02-24,35,316-->
