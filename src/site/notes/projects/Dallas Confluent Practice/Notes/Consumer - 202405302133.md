---
{"dg-publish":true,"permalink":"/projects/dallas-confluent-practice/notes/consumer-202405302133/","tags":["üìñ","‚ô£Ô∏è/kafka"],"created":"2024-03-19T13:32:56.885-05:00","updated":"2024-06-15T09:04:36.894-05:00"}
---

# Consumer


- Consumers read data from a [[projects/Dallas Confluent Practice/Notes/Topics - 202405282249\|Topic]] identified by name.
- Consumers automatically know which [[projects/Dallas Confluent Practice/Notes/Broker - 202405302214\|Broker]] to read from
- In case of [[projects/Dallas Confluent Practice/Notes/Broker - 202405302214\|Broker]] failures, consumers know how to recover
- Data is read in order from low to high offset within each [[projects/Dallas Confluent Practice/Notes/Partitions - 202405282258\|Partition]]
- This is a [[permanent/Pull Model - 202303012046\|Pull Model]]
- Consumption from a single [[projects/Dallas Confluent Practice/Notes/Partitions - 202405282258\|Partition]] can insure order of consumption
- Consumption from multiple [[projects/Dallas Confluent Practice/Notes/Partitions - 202405282258\|Partitions]] does not insure order of consumption


![Pasted image 20230301204659.png](/img/user/projects/Dallas%20Confluent%20Practice/Pasted%20image%2020230301204659.png)

## References


## Flashcards

Consumers read::: These read data from a [[projects/Dallas Confluent Practice/Notes/Topics - 202405282249\|Topic]]
<!--SR:!2024-06-19,14,290!2024-06-21,16,290-->
Consumers know::: These know which [[projects/Dallas Confluent Practice/Notes/Broker - 202405302214\|Broker]] to read from
<!--SR:!2024-06-18,13,290!2024-06-18,13,290-->
Consumers automatically recover on ==broker== failure
<!--SR:!2024-06-02,1,230-->
Consumers read in order from::: This is how what reads from low to high offset with each [[projects/Dallas Confluent Practice/Notes/Partitions - 202405282258\|Partition]]
<!--SR:!2024-07-16,33,290!2024-06-22,17,290-->
Consumers use a ==[[permanent/Pull Model - 202303012046\|Pull Model]]== as it relates to reading from a topic
<!--SR:!2024-06-19,8,238-->

Ordered consumption occurs when reading from ==a single [[projects/Dallas Confluent Practice/Notes/Partitions - 202405282258\|Partition]]==
<!--SR:!2024-06-23,17,298-->

Order consumption is not insured if reading from ==multiple [[projects/Dallas Confluent Practice/Notes/Partitions - 202405282258\|Partitions]]==
<!--SR:!2024-06-02,1,230-->
We would like to be in an at-most once consuming scenario. Which offset commit strategy would you recommend?:: Commit the offsets in Kafka, before processing the data
<!--SR:!2024-06-21,15,298-->

A consumer starts and has auto.offset.reset=latest, and the topic partition currently has data for offsets going from 45 to 2311. The consumer group has committed the offset 643 for the topic before. Where will the consumer read from?:: offset 643
<!--SR:!2024-06-17,12,278-->

You are doing complex calculations using a machine learning framework on records fetched from a Kafka topic. It takes more about 6 minutes to process a record batch, and the consumer enters rebalances even though it's still running. How can you improve this scenario?:: Here, we need to change the setting max.poll.interval.ms (default 300000) to its double in order to tell Kafka a consumer should be considered dead if the consumer only if it hasn't called the .poll() method in 10 minutes instead of 5.
<!--SR:!2024-06-15,2,204-->

How does a consumer commit offsets in Kafka?:: Consumers do not directly write to the `__consumer_offsets` topic, they instead interact with a broker that has been elected to manage that topic, which is the Group Coordinator broker
<!--SR:!2024-06-20,7,284-->

A consumer starts and has auto.offset.reset=none, and the topic partition currently has data for offsets going from 45 to 2311. The consumer group has committed the offset 10 for the topic before. Where will the consumer read from?:: auto.offset.reset=none means that the consumer will crash if the offsets it's recovering from have been deleted from Kafka, which is the case here, as 10 < 45
<!--SR:!2024-06-20,9,284-->

In the Kafka consumer metrics it is observed that fetch-rate is very high and each fetch is small. What steps will you take to increase throughput?:: Increase `fetch.min.bytes` This will allow consumers to wait and receive more bytes in each fetch request.
<!--SR:!2024-06-19,6,244-->

A producer just sent a message to the leader broker for a topic partition. The producer used `acks=1` and therefore the data has not yet been replicated to followers. Under which conditions will the consumer see the message?:: When the high watermark has advanced. The high watermark is an advanced Kafka concept, and is advanced once all the ISR replicates the latest offsets. A consumer can only read up to the value of the High Watermark (which can be less than the highest offset, in the case of `acks=1`)
<!--SR:!2024-06-22,9,264-->

To read data from a topic, the following configuration is needed for the consumers:: any broker to connect to, and the topic name. All brokers can respond to Metadata request, so a client can connect to any broker in the cluster.
<!--SR:!2024-06-15,4,244-->

To allow consumers in a group to resume at the previously committed offset, I need to make sure the consumers in a group use the same...:: group.id Setting a group.id that's consistent across restarts will allow your consumers part of the same group to resume reading from where offsets were last committed for that group
<!--SR:!2024-06-14,3,244-->

There are 3 producers writing to a topic with 5 partitions. There are 10 consumers consuming from the topic as part of the same group. How many consumers will remain idle?:: 5 One consumer per partition assignment will keep 5 consumers idle.
<!--SR:!2024-06-29,18,304-->

A consumer sends a request to commit offset 2000. There is a temporary communication problem, so the broker never gets the request and therefore never responds. Meanwhile, the consumer processed another batch and successfully committed offset 3000. What should you do?:: In this case, because the offset 3000 has been committed and all the messages between 0 and 3000 have all been processed, it is okay not to have committed offset 2000. The right answer is to do "nothing", this behaviour is acceptable
<!--SR:!2024-06-27,16,304-->

A topic has three replicas and you set min.insync.replicas to 2. If two out of three replicas are not available, what happens when a consume request is sent to broker?:: Data will be returned from the remaining in-sync replica
<!--SR:!2024-06-23,12,284-->

Consumer failed to process record # 10 and succeeded in processing record # 11. Select the course of action that you should choose to guarantee at least once processing:: Do not commit until successfully processing the record #10 Here, you shouldn't commit offsets 11 or 10 as it would indicate that the message #10 has been processed successfully.
<!--SR:!2024-06-16,4,287-->

A consumer has auto.offset.reset=latest, and the topic partition currently has data for offsets going from 45 to 2311. The consumer group never committed offsets for the topic before. Where will the consumer read from?:: offset 2311 Latest means that data retrievals will start from where the offsets currently end
<!--SR:!2024-06-16,4,287-->

A consumer is configured with enable.auto.commit=false. What happens when close() is called on the consumer object?:: A rebalance in the consumer group will happen immediately. Calling close() on consumer immediately triggers a partition rebalance as the consumer will not be available anymore.
<!--SR:!2024-06-16,3,246-->

What is a generic unique id that I can use for messages I receive from a consumer?:: topic + partition + offset
<!--SR:!2024-06-15,4,286-->