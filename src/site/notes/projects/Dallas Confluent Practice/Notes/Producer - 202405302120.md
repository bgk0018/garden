---
{"dg-publish":true,"permalink":"/projects/dallas-confluent-practice/notes/producer-202405302120/","tags":["üìñ","‚ô£Ô∏è/kafka"],"created":"2024-03-19T13:32:56.645-05:00","updated":"2024-07-09T17:16:51.521-05:00"}
---

# Producer

Producers write data to [[projects/Dallas Confluent Practice/Notes/Topics - 202405282249\|Topics]]. Producers know to which [[projects/Dallas Confluent Practice/Notes/Partitions - 202405282258\|Partition]] to write to and which [[projects/Dallas Confluent Practice/Notes/Broker - 202405302214\|Broker]] has it.

In case of [[projects/Dallas Confluent Practice/Notes/Broker - 202405302214\|Broker]] failures, Producers will automatically recover.

Kafka scales well because [[projects/Dallas Confluent Practice/Notes/Topics - 202405282249\|Topics]] are split across multiple [[projects/Dallas Confluent Practice/Notes/Partitions - 202405282258\|Partitions]].

Each [[projects/Dallas Confluent Practice/Notes/Topics - 202405282249\|Topic]] will receive messages from one or more Producers.

Producers can choose to send a [[projects/Dallas Confluent Practice/Notes/Message Key - 202303012028\|Message Key]] with the [[projects/Dallas Confluent Practice/Notes/Messages - 202406042227\|Message]].

linger.ms forces the producer to wait to send messages, hence increasing the chance of creating batches

## References


## Flashcards

Producers do this::: write data to [[projects/Dallas Confluent Practice/Notes/Topics - 202405282249\|Topics]]
<!--SR:!2024-08-30,64,310!2024-08-21,55,310-->
Producers know::: This knows which [[projects/Dallas Confluent Practice/Notes/Partitions - 202405282258\|Partitions]] to write to and which [[projects/Dallas Confluent Practice/Notes/Broker - 202405302214\|Broker]] has that partition
<!--SR:!2024-09-21,74,290!2024-08-11,33,290-->
Topics scale by::: being split across multiple [[projects/Dallas Confluent Practice/Notes/Partitions - 202405282258\|Partitions]]
<!--SR:!2024-08-21,55,310!2024-08-21,55,310-->
Producers will automatically recover::: In case of [[projects/Dallas Confluent Practice/Notes/Broker - 202405302214\|Broker]] failures
<!--SR:!2024-08-20,54,310!2024-09-05,70,310-->
A [[projects/Dallas Confluent Practice/Notes/Topics - 202405282249\|Topic]] receives messages from::: one or more [[projects/Dallas Confluent Practice/Notes/Producer - 202405302120\|Producers]]
<!--SR:!2024-09-07,60,310!2024-08-28,62,310-->
Producers can choose to send a ==[[projects/Dallas Confluent Practice/Notes/Message Key - 202303012028\|Message Key]]== with the [[projects/Dallas Confluent Practice/Notes/Messages - 202406042227\|Message]]
<!--SR:!2024-08-26,48,310-->


A Kafka producer application wants to send log messages to a topic that does not include any key. What are the properties that are mandatory to configure for the producer configuration? (select three)
- bootstrap.servers
- partition
- value
- key.serializer
- key
- value.serializer
?
- bootstrap.servers
- key.serializer
- value.serializer
<!--SR:!2024-07-24,15,270-->

What setting increases the chance of batching for a Kafka Producer?:: linger.ms
<!--SR:!2024-09-18,71,320-->

To enhance compression, I can increase the chances of batching by using:: linger.ms forces the producer to wait before sending messages, hence increasing the chance of creating batches that can be heavily compressed.
<!--SR:!2024-09-13,78,320-->

A customer has many consumer applications that process messages from a Kafka topic. Each consumer application can only process 50 MB/s. Your customer wants to achieve a target throughput of 1 GB/s. What is the minimum number of partitions will you suggest to the customer for that particular topic?:: each consumer can process only 50 MB/s, so we need at least 20 consumers consuming one partition so that 50 * 20 = 1000 MB target is achieved.
<!--SR:!2024-09-10,75,320-->

Your producer is producing at a very high rate and the batches are completely full each time. How can you improve the producer throughput? (select two)
- Decrease linger.ms
- Increase batch.size
- Disable compression
- Decrease batch.size
- Enable compression
- Increase linger.ms
?
- Increase batch.size
- Enable compression
batch.size controls how many bytes of data to collect before sending messages to the Kafka broker. Set this as high as possible, without exceeding available memory. Enabling compression can also help make more compact batches and increase the throughput of your producer. Linger.ms will have no effect as the batches are already full
<!--SR:!2024-08-08,42,300-->

What is the risk of increasing `max.in.flight.requests.per.connection` while also enabling retries in a producer?:: Message order not preserved
<!--SR:!2024-09-08,73,320-->

You are sending messages with keys to a topic. To increase throughput, you decide to increase the number of partitions of the topic. Select all that apply.
- New records with the same key will get written to the partition where old records with that key were written
- All the existing records will get rebalanced among the partitions to balance load
- Old records will stay in their partitions
- New records may get written to a different partition
?
- Old records will stay in their partitions
- New records may get written to a different partition
Increasing the number of partition causes new messages keys to get hashed differently, and breaks the guarantee "same keys goes to the same partition". Kafka logs are immutable and the previous messages are not re-shuffled
<!--SR:!2024-09-23,76,330-->

A kafka topic has a replication factor of 3 and min.insync.replicas setting of 2. How many brokers can go down before a producer with acks=all can't produce?:: 1 acks=all and min.insync.replicas=2 means we must have at least 2 brokers up for the partition to be available
<!--SR:!2024-09-01,66,330-->

To prevent network-induced duplicates when producing to Kafka, I should use:: enable.idempotence=true Producer idempotence helps prevent the network introduced duplicates
<!--SR:!2024-07-21,12,250-->

The rule "same key goes to the same partition" is true unless...:: the number of partition changes. Increasing the number of partition causes new messages keys to get hashed differently, and breaks the guarantee "same keys goes to the same partition". Kafka logs are immutable and the previous messages are not re-shuffled.
<!--SR:!2024-09-23,76,330-->

If I supply the setting compression.type=snappy to my producer, what will happen? (select two)
- The Consumers have to compress the data
- The Producers have to compress the data
- The Kafka brokers have to compress the data
- The Consumers have to de-compress the data
- The Kafka brokers have to de-compress the data
?
- The Producers have to compress the data
- The Consumers have to de-compress the data
Kafka transfers data with zero copy and no transformation. Any transformation (including compression) is the responsibility of clients.
<!--SR:!2024-07-16,19,318-->

You want to send a message of size 3 MB to a topic with default message size configuration. How does KafkaProducer handle large messages?:: MessageSizeTooLarge exception will be thrown, KafkaProducer will not retry and return exception immediately
<!--SR:!2024-07-25,16,318-->

To produce data to a topic, a producer must provide the Kafka client with...:: any broker from the cluster and the topic name. All brokers can respond to a Metadata request, so a client can connect to any broker in the cluster and then figure out on its own which brokers to send data to.
<!--SR:!2024-07-15,18,318-->


What happens if you write the following code in your producer? producer.send(producerRecord).get():: Throughput will be decreased Using Future.get() to wait for a reply from Kafka will limit throughput.
<!--SR:!2024-07-18,9,298-->

You are receiving orders from different customer in an "orders" topic with multiple partitions. Each message has the customer name as the key. There is a special customer named ABC that generates a lot of orders and you would like to reserve a partition exclusively for ABC. The rest of the message should be distributed among other partitions. How can this be achieved?:: Create a custom partitioner. A Custom Partitioner allows you to easily customise how the partition number gets computed from a source message.
<!--SR:!2024-07-13,16,298-->

A kafka topic has a replication factor of 3 and min.insync.replicas setting of 2. How many brokers can go down before a producer with acks=1 can't produce?:: 2 min.insync.replicas does not impact producers when acks=1 (only when acks=all)
<!--SR:!2024-07-14,17,298-->

A topic has three replicas and you set min.insync.replicas to 2. If two out of three replicas are not available, what happens when a produce request with acks=all is sent to broker?:: NotEnoughReplicasException will be returned. With this configuration, a single in-sync replica becomes read-only. Produce request will receive NotEnoughReplicasException.
<!--SR:!2024-07-28,19,318-->