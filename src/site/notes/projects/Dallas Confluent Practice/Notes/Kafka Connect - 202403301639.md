---
{"dg-publish":true,"permalink":"/projects/dallas-confluent-practice/notes/kafka-connect-202403301639/","tags":["üìñ","‚ô£Ô∏è/kafka"],"created":"2024-03-31T02:14:48.612-05:00","updated":"2024-07-09T17:22:23.954-05:00"}
---

# Kafka Connect

Solves external source to [[Kafka\|Kafka]] and [[Kafka\|Kafka]] to an external sink.

## References


## Flashcards

What is the connect-status topic?:: Helps to elect leaders for connect
<!--SR:!2024-07-11,14,230-->

What is the connect-configs topic?:: Stores configuration
<!--SR:!2024-08-18,52,310-->

What is the connect-offsets topic?:: Store source offsets for source connectors
<!--SR:!2024-07-13,4,250-->

When using plain JSON data with Connect, you see the following error message: org.apache.kafka.connect.errors.DataException: JsonDeserializer with schemas.enable requires "schema" and "payload" fields and may not contain additional fields. How will you fix the error?:: Set key.converter.schemas.enable and value.converter.schemas.enable to false
<!--SR:!2024-07-14,17,268-->

To continuously export data from Kafka into a target database, I should use:: Kafka Connect Sink is used to export data from Kafka to external databases and Kafka Connect Source is used to import from external databases into Kafka.
<!--SR:!2024-07-13,16,293-->